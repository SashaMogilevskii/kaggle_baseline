{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nfrom  tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, Dataset, random_split, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import get_linear_schedule_with_warmup, AdamW, AutoTokenizer\nfrom sklearn.metrics import accuracy_score \n\n\nfrom transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, AutoModelForSequenceClassification\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:45:34.683600Z","iopub.execute_input":"2022-08-06T20:45:34.684042Z","iopub.status.idle":"2022-08-06T20:45:42.899137Z","shell.execute_reply.started":"2022-08-06T20:45:34.683955Z","shell.execute_reply":"2022-08-06T20:45:42.897985Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print('Thera are  %d GPU(s) available.' % torch.cuda.device_count())\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:45:42.901697Z","iopub.execute_input":"2022-08-06T20:45:42.902728Z","iopub.status.idle":"2022-08-06T20:45:42.968248Z","shell.execute_reply.started":"2022-08-06T20:45:42.902679Z","shell.execute_reply":"2022-08-06T20:45:42.967251Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Thera are  1 GPU(s) available.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# model_name = \"microsoft/deberta-v2-xlarge\"\n# model_name = \"microsoft/deberta-v3-base\"\nmodel_name = \"microsoft/deberta-v3-base\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, \n                                                           num_labels=3,\n                                                          output_attentions = False,\n                                                            output_hidden_states = False,).to(device)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmax_length = 64\ntokenizer.model_max_length = max_length","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:45:42.969916Z","iopub.execute_input":"2022-08-06T20:45:42.970857Z","iopub.status.idle":"2022-08-06T20:46:18.302182Z","shell.execute_reply.started":"2022-08-06T20:45:42.970819Z","shell.execute_reply":"2022-08-06T20:46:18.300996Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0900d9b81d04810824e004cd2f14c40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"513b82b282a24fdd9e4f66af8214dc5c"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76ae916dc9164480a2f3954066dc1218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5b5728624446ae8a3b92cc1dfc04bd"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n/opt/conda/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('../input/feedback-prize-effectiveness/train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:18.305026Z","iopub.execute_input":"2022-08-06T20:46:18.305690Z","iopub.status.idle":"2022-08-06T20:46:18.548612Z","shell.execute_reply.started":"2022-08-06T20:46:18.305650Z","shell.execute_reply":"2022-08-06T20:46:18.547595Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def normalise(text):\n#     text = text.lower()\n#     text = text.strip()\n#     text = re.sub(\"\\n\", \" \", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:18.549919Z","iopub.execute_input":"2022-08-06T20:46:18.551234Z","iopub.status.idle":"2022-08-06T20:46:18.557854Z","shell.execute_reply.started":"2022-08-06T20:46:18.551196Z","shell.execute_reply":"2022-08-06T20:46:18.555369Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df[\"essay_text\"] = df[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/train/{x}.txt').read())\n\ndf['discourse_type'] = df['discourse_type'].apply(normalise)\ndf['discourse_text'] = df['discourse_text'].apply(normalise)\ndf['essay_text'] = df['essay_text'].apply(normalise)\n\ndf['text_features'] =  df['discourse_type'] + tokenizer.sep_token + df['discourse_text'] + tokenizer.sep_token + df['essay_text']\ndf.drop(['discourse_id', 'essay_id', 'essay_text', 'discourse_text', 'discourse_type'], axis=1, inplace=True )","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:18.559428Z","iopub.execute_input":"2022-08-06T20:46:18.560141Z","iopub.status.idle":"2022-08-06T20:46:42.149612Z","shell.execute_reply.started":"2022-08-06T20:46:18.560106Z","shell.execute_reply":"2022-08-06T20:46:42.148629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"classes_to_labels = {\n    \"Adequate\":1,\n    \"Effective\":2,\n    \"Ineffective\":0,\n}\ndf['discourse_effectiveness'] = df['discourse_effectiveness'].map(classes_to_labels)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.151238Z","iopub.execute_input":"2022-08-06T20:46:42.151601Z","iopub.status.idle":"2022-08-06T20:46:42.161045Z","shell.execute_reply.started":"2022-08-06T20:46:42.151565Z","shell.execute_reply":"2022-08-06T20:46:42.160076Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# w_adequate = 1-len(df[df['discourse_effectiveness'] == 'adequate'])/len(df)\n# w_effective = 1-len(df[df['discourse_effectiveness'] == 'effective'])/len(df)\n# w_ineffective = 1-len(df[df['discourse_effectiveness'] == 'ineffective'])/len(df)\nw_adequate = 1\nw_effective = 1\nw_ineffective = 1","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.162559Z","iopub.execute_input":"2022-08-06T20:46:42.163208Z","iopub.status.idle":"2022-08-06T20:46:42.171710Z","shell.execute_reply.started":"2022-08-06T20:46:42.163173Z","shell.execute_reply":"2022-08-06T20:46:42.170704Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.173072Z","iopub.execute_input":"2022-08-06T20:46:42.173642Z","iopub.status.idle":"2022-08-06T20:46:42.192181Z","shell.execute_reply.started":"2022-08-06T20:46:42.173573Z","shell.execute_reply":"2022-08-06T20:46:42.191198Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   discourse_effectiveness                                      text_features\n0                        1  Lead[SEP]Hi, i'm Isaac, i'm going to be writin...\n1                        1  Position[SEP]On my perspective, I think that t...\n2                        1  Claim[SEP]I think that the face is a natural l...\n3                        1  Evidence[SEP]If life was on Mars, we would kno...\n4                        1  Counterclaim[SEP]People thought that the face ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>discourse_effectiveness</th>\n      <th>text_features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>Lead[SEP]Hi, i'm Isaac, i'm going to be writin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Position[SEP]On my perspective, I think that t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Claim[SEP]I think that the face is a natural l...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Evidence[SEP]If life was on Mars, we would kno...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>Counterclaim[SEP]People thought that the face ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sentences = df.text_features.values\nlabels = df.discourse_effectiveness.values","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.196403Z","iopub.execute_input":"2022-08-06T20:46:42.197122Z","iopub.status.idle":"2022-08-06T20:46:42.202319Z","shell.execute_reply.started":"2022-08-06T20:46:42.197088Z","shell.execute_reply":"2022-08-06T20:46:42.201416Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.203652Z","iopub.execute_input":"2022-08-06T20:46:42.204093Z","iopub.status.idle":"2022-08-06T20:46:42.211441Z","shell.execute_reply.started":"2022-08-06T20:46:42.204059Z","shell.execute_reply":"2022-08-06T20:46:42.210427Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# def Token_example( list_of_texts = None, index = 0):\n#     if list_of_texts is None:\n#         list_of_texts=['It is he' + str(tokenizer.sep_token) +'He is labrador. His name is Zeus ' ]\n#     print('Original:', list_of_texts[index])\n#     print()\n#     print('With Token:', tokenizer.tokenize(list_of_texts[index]))\n#     print()\n#     print('With Token IDs:', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(list_of_texts[index])))\n#     print()\n# Token_example(sentences)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.213002Z","iopub.execute_input":"2022-08-06T20:46:42.213336Z","iopub.status.idle":"2022-08-06T20:46:42.221080Z","shell.execute_reply.started":"2022-08-06T20:46:42.213304Z","shell.execute_reply":"2022-08-06T20:46:42.220010Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\ninput_ids = []\nattention_masks = []\nfor sent in tqdm(sentences):\n    encoded_dict = tokenizer.encode_plus(\n        sent, \n        add_special_tokens = True,\n        max_length = max_length,\n        pad_to_max_length = True,\n        return_attention_mask = True,\n        return_tensors = 'pt'\n    )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\ny = torch.tensor(labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:46:42.224218Z","iopub.execute_input":"2022-08-06T20:46:42.224498Z","iopub.status.idle":"2022-08-06T20:48:22.063066Z","shell.execute_reply.started":"2022-08-06T20:46:42.224475Z","shell.execute_reply":"2022-08-06T20:48:22.062050Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"  0%|          | 0/36765 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 36765/36765 [01:39<00:00, 368.95it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\n\ndevice_lib.list_local_devices()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.064654Z","iopub.execute_input":"2022-08-06T20:48:22.065023Z","iopub.status.idle":"2022-08-06T20:48:22.094566Z","shell.execute_reply.started":"2022-08-06T20:48:22.064988Z","shell.execute_reply":"2022-08-06T20:48:22.093242Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2022-08-06 20:48:22.067956: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-08-06 20:48:22.075734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.076876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.077549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.079594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.080339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.081052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-08-06 20:48:22.081646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 14289 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[name: \"/device:CPU:0\"\n device_type: \"CPU\"\n memory_limit: 268435456\n locality {\n }\n incarnation: 8022217707440271652,\n name: \"/device:GPU:0\"\n device_type: \"GPU\"\n memory_limit: 14983888896\n locality {\n   bus_id: 1\n   links {\n   }\n }\n incarnation: 862300317275464188\n physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"},"metadata":{}}]},{"cell_type":"code","source":"dataset = TensorDataset(input_ids, attention_masks, y)\nsize_train = 0.95\ntrain_size = int( size_train * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.096220Z","iopub.execute_input":"2022-08-06T20:48:22.096762Z","iopub.status.idle":"2022-08-06T20:48:22.107466Z","shell.execute_reply.started":"2022-08-06T20:48:22.096726Z","shell.execute_reply":"2022-08-06T20:48:22.106496Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"batch_size = 40\ntrain_dataloader = DataLoader(\n    train_dataset,\n    sampler = RandomSampler(train_dataset),\n    batch_size = batch_size\n)\n\ntest_dataloader = DataLoader(\n    test_dataset,\n    sampler = SequentialSampler(test_dataset),\n    batch_size = batch_size\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.109054Z","iopub.execute_input":"2022-08-06T20:48:22.109584Z","iopub.status.idle":"2022-08-06T20:48:22.116251Z","shell.execute_reply.started":"2022-08-06T20:48:22.109541Z","shell.execute_reply":"2022-08-06T20:48:22.115275Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW( model.parameters(),\n                  lr = 2e-5,\n                  eps = 1e-8\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.117703Z","iopub.execute_input":"2022-08-06T20:48:22.118341Z","iopub.status.idle":"2022-08-06T20:48:22.132436Z","shell.execute_reply.started":"2022-08-06T20:48:22.118307Z","shell.execute_reply":"2022-08-06T20:48:22.131182Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"code","source":"epochs = 2\n\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup( optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps= total_steps)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.133725Z","iopub.execute_input":"2022-08-06T20:48:22.134243Z","iopub.status.idle":"2022-08-06T20:48:22.140525Z","shell.execute_reply.started":"2022-08-06T20:48:22.134209Z","shell.execute_reply":"2022-08-06T20:48:22.139338Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np\n\n\nseed_val = 42\n\nrandom.seed(42)\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.141944Z","iopub.execute_input":"2022-08-06T20:48:22.142536Z","iopub.status.idle":"2022-08-06T20:48:22.150034Z","shell.execute_reply.started":"2022-08-06T20:48:22.142503Z","shell.execute_reply":"2022-08-06T20:48:22.148750Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n\n# class_weights = torch.tensor(\n#     [w_adequate, w_effective, w_ineffective]\n# ).to(device)\n\n# loss_fct = nn.CrossEntropyLoss(weight=class_weights)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:48:22.151741Z","iopub.execute_input":"2022-08-06T20:48:22.152140Z","iopub.status.idle":"2022-08-06T20:48:22.158989Z","shell.execute_reply.started":"2022-08-06T20:48:22.152108Z","shell.execute_reply":"2022-08-06T20:48:22.158139Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"training_stats = []\n\nfor epoch_i in (range(1, epochs + 1)):\n    total_train_loss = 0\n    model.train()\n    \n    \n    for batch in tqdm(train_dataloader):\n        \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        model.zero_grad()\n        \n        \n        res = model(b_input_ids,\n                   token_type_ids=None,\n                   attention_mask=b_input_mask,\n                   labels = b_labels)\n            \n        loss= res['loss']\n        logits = res['logits']\n        \n        total_train_loss += loss.item()\n        \n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n    avg_train_loss = total_train_loss / len(train_dataloader)\n    \n    print(f'avg_train_loss:{avg_train_loss}')\n    model.eval()\n    \n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    nb_eval_steps = 0\n    \n    list_of_logits = None\n    list_of_label_ids = None\n    for batch in test_dataloader:\n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n        \n        with torch.no_grad():\n            res = model(b_input_ids,\n                   token_type_ids=None,\n                   attention_mask=b_input_mask,\n                   labels = b_labels) \n            \n            loss= res['loss']\n            logits = res['logits']\n             \n        total_eval_loss += loss.item()\n\n\n#         logits = logits.detach().to('cpu').numpy()\n#         label_ids = b_labels.to('cpu').numpy()\n        \n        logits = logits.detach().to('cpu')\n        label_ids = b_labels.to('cpu')\n\n\n\n        if list_of_logits is None:\n            list_of_logits = logits\n            list_of_label_ids = label_ids\n        else:\n            list_of_logits = torch.cat((list_of_logits, logits))\n            list_of_label_ids = torch.cat((list_of_label_ids, label_ids))\n        \n#         idx = label_ids\n#         pred_true_labels = list_of_logits[torch.arange(len(idx)), idx]\n    pred  = torch.Tensor(list_of_logits).argmax(dim=1).to('cpu')\n\n    acc = accuracy_score(list_of_label_ids, pred)\n    avg_val_loss = total_eval_loss / len(test_dataloader)\n\n    training_stats.append({\n        'epoch': epoch_i,\n        'training_loss': avg_train_loss,\n        'valid.loss': avg_val_loss,\n        'valid.accuracy': acc,})\n    print(training_stats[-1])\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:59:16.562394Z","iopub.execute_input":"2022-08-06T20:59:16.562753Z","iopub.status.idle":"2022-08-06T21:09:50.817125Z","shell.execute_reply.started":"2022-08-06T20:59:16.562723Z","shell.execute_reply":"2022-08-06T21:09:50.815140Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"100%|██████████| 874/874 [05:12<00:00,  2.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"avg_train_loss:0.5912910996162646\n{'epoch': 1, 'training_loss': 0.5912910996162646, 'valid.loss': 0.6629241614238076, 'valid.accuracy': 0.7052746057640021}\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 874/874 [05:12<00:00,  2.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"avg_train_loss:0.5921527829950407\n{'epoch': 2, 'training_loss': 0.5921527829950407, 'valid.loss': 0.6629241614238076, 'valid.accuracy': 0.7052746057640021}\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"model_Deberta_V3_large_lower_weigth.h5\")\ntokenizer.save_pretrained(\"BertDebrta_V3_large_lower_weigth.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-08-06T21:09:50.819094Z","iopub.execute_input":"2022-08-06T21:09:50.819465Z","iopub.status.idle":"2022-08-06T21:09:53.622221Z","shell.execute_reply.started":"2022-08-06T21:09:50.819423Z","shell.execute_reply":"2022-08-06T21:09:53.621263Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"('BertDebrta_V3_large_lower_weigth.h5/tokenizer_config.json',\n 'BertDebrta_V3_large_lower_weigth.h5/special_tokens_map.json',\n 'BertDebrta_V3_large_lower_weigth.h5/spm.model',\n 'BertDebrta_V3_large_lower_weigth.h5/added_tokens.json',\n 'BertDebrta_V3_large_lower_weigth.h5/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.529179Z","iopub.execute_input":"2022-08-06T20:58:59.531723Z","iopub.status.idle":"2022-08-06T20:58:59.548919Z","shell.execute_reply.started":"2022-08-06T20:58:59.531686Z","shell.execute_reply":"2022-08-06T20:58:59.547827Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df[\"essay_text\"] = df[\"essay_id\"].apply(lambda x: open(f'../input/feedback-prize-effectiveness/test/{x}.txt').read())\n\ndf['discourse_type'] = df['discourse_type'].apply(normalise)\ndf['discourse_text'] = df['discourse_text'].apply(normalise)\ndf['essay_text'] = df['essay_text'].apply(normalise)\n\ndf['text_features'] =  df['discourse_type'] + tokenizer.sep_token + df['discourse_text'] + tokenizer.sep_token + df['essay_text']\ndf.drop(['discourse_id', 'essay_id', 'essay_text', 'discourse_text', 'discourse_type'], axis=1, inplace=True )","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.550336Z","iopub.execute_input":"2022-08-06T20:58:59.550846Z","iopub.status.idle":"2022-08-06T20:58:59.572200Z","shell.execute_reply.started":"2022-08-06T20:58:59.550809Z","shell.execute_reply":"2022-08-06T20:58:59.571352Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"sentences = df.text_features.values\ndel df","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.575212Z","iopub.execute_input":"2022-08-06T20:58:59.576078Z","iopub.status.idle":"2022-08-06T20:58:59.583683Z","shell.execute_reply.started":"2022-08-06T20:58:59.576045Z","shell.execute_reply":"2022-08-06T20:58:59.582814Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"\n\ninput_ids = []\nattention_masks = []\nfor sent in tqdm(sentences):\n    encoded_dict = tokenizer.encode_plus(\n        sent, \n        add_special_tokens = True,\n        max_length = max_length,\n        pad_to_max_length = True,\n        return_attention_mask = True,\n        return_tensors = 'pt'\n    )\n    \n    input_ids.append(encoded_dict['input_ids'])\n    attention_masks.append(encoded_dict['attention_mask'])\n\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.585171Z","iopub.execute_input":"2022-08-06T20:58:59.585918Z","iopub.status.idle":"2022-08-06T20:58:59.639665Z","shell.execute_reply.started":"2022-08-06T20:58:59.585884Z","shell.execute_reply":"2022-08-06T20:58:59.638811Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n100%|██████████| 10/10 [00:00<00:00, 282.03it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"dataset = TensorDataset(input_ids, attention_masks)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.643765Z","iopub.execute_input":"2022-08-06T20:58:59.646117Z","iopub.status.idle":"2022-08-06T20:58:59.652124Z","shell.execute_reply.started":"2022-08-06T20:58:59.646079Z","shell.execute_reply":"2022-08-06T20:58:59.651183Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"batch_size = 36\n\nprediction_dataloader = DataLoader(\n    dataset,\n    sampler = SequentialSampler(dataset),\n    batch_size = batch_size\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.660872Z","iopub.execute_input":"2022-08-06T20:58:59.663015Z","iopub.status.idle":"2022-08-06T20:58:59.671149Z","shell.execute_reply.started":"2022-08-06T20:58:59.662977Z","shell.execute_reply":"2022-08-06T20:58:59.670202Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.eval()\npredictions = None\nm = nn.Softmax(dim=1)\nfor batch in prediction_dataloader:\n    b_inputs_ids, b_input_mask = batch\n    b_inputs_ids = b_inputs_ids.to(device)\n    b_input_mask = b_input_mask.to(device)\n    with torch.no_grad():\n        outputs = model(b_inputs_ids, token_type_ids=None,\n                       attention_mask=b_input_mask)\n        \n        outputs = m(outputs.logits)\n    if predictions == None:\n        predictions = outputs\n    else:\n\n        predictions = torch.cat((predictions, outputs))\npredictions = predictions.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.674988Z","iopub.execute_input":"2022-08-06T20:58:59.676395Z","iopub.status.idle":"2022-08-06T20:58:59.728189Z","shell.execute_reply.started":"2022-08-06T20:58:59.676361Z","shell.execute_reply":"2022-08-06T20:58:59.726123Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"predictions","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.730507Z","iopub.execute_input":"2022-08-06T20:58:59.731464Z","iopub.status.idle":"2022-08-06T20:58:59.742741Z","shell.execute_reply.started":"2022-08-06T20:58:59.731401Z","shell.execute_reply":"2022-08-06T20:58:59.741397Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[0.0108, 0.3510, 0.6382],\n        [0.0143, 0.5378, 0.4479],\n        [0.0044, 0.2698, 0.7258],\n        [0.0074, 0.3230, 0.6696],\n        [0.0123, 0.4910, 0.4967],\n        [0.0030, 0.1448, 0.8522],\n        [0.0069, 0.2467, 0.7465],\n        [0.0083, 0.4240, 0.5676],\n        [0.0031, 0.1533, 0.8436],\n        [0.0113, 0.3947, 0.5939]])"},"metadata":{}}]},{"cell_type":"code","source":"submit = pd.read_csv('/kaggle/input/feedback-prize-effectiveness/sample_submission.csv')\nsubmission = pd.DataFrame({'discourse_id':submit['discourse_id'],'Adequate':predictions[:,1],'Effective':predictions[:,2],'Ineffective':predictions[:,0]})\nsubmission.to_csv(\"/kaggle/working/submission.csv\",index = False)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T20:58:59.744345Z","iopub.execute_input":"2022-08-06T20:58:59.748530Z","iopub.status.idle":"2022-08-06T20:58:59.772060Z","shell.execute_reply.started":"2022-08-06T20:58:59.748487Z","shell.execute_reply":"2022-08-06T20:58:59.768217Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}